{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Architecture Analysis for DC Decomposition\n",
    "\n",
    "This notebook analyzes the Vision Transformer (ViT) architecture to identify:\n",
    "1. All module types used\n",
    "2. Operations that need special handling (division, reshape, etc.)\n",
    "3. How to implement DC decomposition hooks for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "# Check if transformers is available\n",
    "try:\n",
    "    from transformers import ViTConfig, ViTModel, ViTForImageClassification\n",
    "    HAS_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    print(\"Install transformers: pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load ViT Model with ReLU Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    # Load ViT with ReLU instead of GELU\n",
    "    config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "    config.hidden_act = \"relu\"  # Override GELU with ReLU\n",
    "    \n",
    "    model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\", config=config)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded: {type(model).__name__}\")\n",
    "    print(f\"Hidden size: {config.hidden_size}\")\n",
    "    print(f\"Num attention heads: {config.num_attention_heads}\")\n",
    "    print(f\"Num hidden layers: {config.num_hidden_layers}\")\n",
    "    print(f\"Intermediate size: {config.intermediate_size}\")\n",
    "    print(f\"Activation: {config.hidden_act}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List All Module Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    # Collect all module types\n",
    "    module_types = defaultdict(list)\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        module_type = type(module).__name__\n",
    "        module_types[module_type].append(name)\n",
    "    \n",
    "    print(\"Module Types in ViT:\")\n",
    "    print(\"=\" * 60)\n",
    "    for mtype, names in sorted(module_types.items()):\n",
    "        print(f\"\\n{mtype} ({len(names)} instances):\")\n",
    "        for name in names[:3]:  # Show first 3\n",
    "            print(f\"  - {name}\")\n",
    "        if len(names) > 3:\n",
    "            print(f\"  ... and {len(names) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detailed Module Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    print(\"Detailed Module Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Categorize modules by DC decomposition support\n",
    "    supported = []\n",
    "    needs_implementation = []\n",
    "    container_modules = []\n",
    "    \n",
    "    supported_types = (nn.Linear, nn.LayerNorm, nn.ReLU, nn.Softmax, \n",
    "                       nn.Conv2d, nn.Dropout, nn.Identity)\n",
    "    container_types = (nn.ModuleList, nn.Sequential)\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\":\n",
    "            continue\n",
    "        mtype = type(module).__name__\n",
    "        \n",
    "        if isinstance(module, supported_types):\n",
    "            supported.append((name, mtype))\n",
    "        elif isinstance(module, container_types):\n",
    "            container_modules.append((name, mtype))\n",
    "        elif mtype in ['ViTModel', 'ViTEncoder', 'ViTEmbeddings', 'ViTLayer', \n",
    "                       'ViTAttention', 'ViTSelfAttention', 'ViTSelfOutput',\n",
    "                       'ViTIntermediate', 'ViTOutput', 'ViTPatchEmbeddings',\n",
    "                       'ViTPooler']:\n",
    "            container_modules.append((name, mtype))\n",
    "        else:\n",
    "            needs_implementation.append((name, mtype))\n",
    "    \n",
    "    print(f\"\\nSupported modules: {len(supported)}\")\n",
    "    print(f\"Container modules: {len(container_modules)}\")\n",
    "    print(f\"Needs implementation: {len(needs_implementation)}\")\n",
    "    \n",
    "    if needs_implementation:\n",
    "        print(\"\\nModules needing implementation:\")\n",
    "        for name, mtype in needs_implementation:\n",
    "            print(f\"  {mtype}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze ViT Attention Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    # Let's look at the attention module's forward pass\n",
    "    import inspect\n",
    "    \n",
    "    # Get attention layer\n",
    "    attn_layer = model.encoder.layer[0].attention.attention\n",
    "    \n",
    "    print(\"ViTSelfAttention Structure:\")\n",
    "    print(\"=\" * 60)\n",
    "    for name, child in attn_layer.named_children():\n",
    "        print(f\"  {name}: {type(child).__name__}\")\n",
    "        if isinstance(child, nn.Linear):\n",
    "            print(f\"    in_features={child.in_features}, out_features={child.out_features}\")\n",
    "    \n",
    "    print(f\"\\nAttention parameters:\")\n",
    "    print(f\"  num_attention_heads: {attn_layer.num_attention_heads}\")\n",
    "    print(f\"  attention_head_size: {attn_layer.attention_head_size}\")\n",
    "    print(f\"  all_head_size: {attn_layer.all_head_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    # Trace forward pass to identify operations\n",
    "    print(\"\\nViT Attention Forward Operations:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    1. query = self.query(hidden_states)  # Linear\n",
    "    2. key = self.key(hidden_states)      # Linear\n",
    "    3. value = self.value(hidden_states)  # Linear\n",
    "    \n",
    "    4. query = transpose_for_scores(query)  # Reshape + Permute\n",
    "       - shape: (batch, seq, hidden) -> (batch, heads, seq, head_dim)\n",
    "    \n",
    "    5. attention_scores = query @ key.transpose(-1, -2)  # MatMul\n",
    "    \n",
    "    6. attention_scores = attention_scores / sqrt(head_dim)  # Division by scalar\n",
    "    \n",
    "    7. attention_probs = softmax(attention_scores, dim=-1)  # Softmax\n",
    "    \n",
    "    8. attention_probs = dropout(attention_probs)  # Dropout (identity in eval)\n",
    "    \n",
    "    9. context = attention_probs @ value  # MatMul\n",
    "    \n",
    "    10. context = context.permute(0, 2, 1, 3).contiguous()  # Permute\n",
    "    \n",
    "    11. context = context.view(batch, seq, hidden)  # Reshape\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nOperations needing DC decomposition support:\")\n",
    "    print(\"  - MatMul (Q @ K^T, attn @ V): DCMatMul ✓\")\n",
    "    print(\"  - Division by scalar: DCScalarDiv (new)\")\n",
    "    print(\"  - Reshape/View: DCReshape (new)\")\n",
    "    print(\"  - Permute/Transpose: DCPermute (new)\")\n",
    "    print(\"  - Softmax: Already supported ✓\")\n",
    "    print(\"  - Dropout: Identity in eval mode ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Missing DC Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC modules for operations that need special handling\n",
    "\n",
    "class DCReshape(nn.Module):\n",
    "    \"\"\"\n",
    "    Reshape module for DC decomposition.\n",
    "    \n",
    "    Reshape is a linear operation - it just changes the view of data.\n",
    "    Both pos and neg streams are reshaped identically.\n",
    "    \"\"\"\n",
    "    _dc_is_reshape = True\n",
    "    \n",
    "    def __init__(self, target_shape):\n",
    "        super().__init__()\n",
    "        self.target_shape = target_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.view(*self.target_shape)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'target_shape={self.target_shape}'\n",
    "\n",
    "\n",
    "class DCPermute(nn.Module):\n",
    "    \"\"\"\n",
    "    Permute module for DC decomposition.\n",
    "    \n",
    "    Permute is a linear operation - it just reorders dimensions.\n",
    "    Both pos and neg streams are permuted identically.\n",
    "    \"\"\"\n",
    "    _dc_is_permute = True\n",
    "    \n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.permute(*self.dims)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'dims={self.dims}'\n",
    "\n",
    "\n",
    "class DCTranspose(nn.Module):\n",
    "    \"\"\"\n",
    "    Transpose module for DC decomposition.\n",
    "    \n",
    "    Transpose is a linear operation.\n",
    "    Both pos and neg streams are transposed identically.\n",
    "    \"\"\"\n",
    "    _dc_is_transpose = True\n",
    "    \n",
    "    def __init__(self, dim0, dim1):\n",
    "        super().__init__()\n",
    "        self.dim0 = dim0\n",
    "        self.dim1 = dim1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.transpose(self.dim0, self.dim1)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'dim0={self.dim0}, dim1={self.dim1}'\n",
    "\n",
    "\n",
    "class DCScalarMul(nn.Module):\n",
    "    \"\"\"\n",
    "    Scalar multiplication for DC decomposition.\n",
    "    \n",
    "    For scalar s:\n",
    "    - If s >= 0: pos_out = s * pos_in, neg_out = s * neg_in\n",
    "    - If s < 0: pos_out = |s| * neg_in, neg_out = |s| * pos_in\n",
    "    \"\"\"\n",
    "    _dc_is_scalar_mul = True\n",
    "    \n",
    "    def __init__(self, scalar):\n",
    "        super().__init__()\n",
    "        self.register_buffer('scalar', torch.tensor(scalar))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.scalar\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'scalar={self.scalar.item()}'\n",
    "\n",
    "\n",
    "class DCScalarDiv(nn.Module):\n",
    "    \"\"\"\n",
    "    Scalar division for DC decomposition.\n",
    "    \n",
    "    Division by positive scalar s is equivalent to multiplication by 1/s.\n",
    "    pos_out = pos_in / s, neg_out = neg_in / s\n",
    "    \"\"\"\n",
    "    _dc_is_scalar_div = True\n",
    "    \n",
    "    def __init__(self, scalar):\n",
    "        super().__init__()\n",
    "        self.register_buffer('scalar', torch.tensor(scalar))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x / self.scalar\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'scalar={self.scalar.item()}'\n",
    "\n",
    "\n",
    "class DCAdd(nn.Module):\n",
    "    \"\"\"\n",
    "    Element-wise addition for DC decomposition.\n",
    "    \n",
    "    (a_pos - a_neg) + (b_pos - b_neg) = (a_pos + b_pos) - (a_neg + b_neg)\n",
    "    \"\"\"\n",
    "    _dc_is_add = True\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "\n",
    "class DCContiguous(nn.Module):\n",
    "    \"\"\"\n",
    "    Make tensor contiguous for DC decomposition.\n",
    "    \n",
    "    This is a no-op mathematically but ensures memory layout.\n",
    "    \"\"\"\n",
    "    _dc_is_contiguous = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.contiguous()\n",
    "\n",
    "\n",
    "print(\"DC Operation Modules defined:\")\n",
    "print(\"  - DCReshape: view/reshape operations\")\n",
    "print(\"  - DCPermute: dimension permutation\")\n",
    "print(\"  - DCTranspose: dimension transposition\")\n",
    "print(\"  - DCScalarMul: multiplication by scalar\")\n",
    "print(\"  - DCScalarDiv: division by scalar\")\n",
    "print(\"  - DCAdd: element-wise addition\")\n",
    "print(\"  - DCContiguous: memory layout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create DC-Compatible ViT Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from dc_decompose import DCMatMul\n",
    "\n",
    "\n",
    "class DCViTSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT Self-Attention with DC-decomposable operations.\n",
    "    \n",
    "    All operations are wrapped in modules that can be hooked:\n",
    "    - Linear projections (query, key, value)\n",
    "    - Reshape operations\n",
    "    - Matrix multiplications\n",
    "    - Scalar division\n",
    "    - Softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        \n",
    "        # Linear projections\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        \n",
    "        # DC-compatible operations\n",
    "        self.scale_div = DCScalarDiv(self.attention_head_size ** 0.5)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        \n",
    "        # MatMul modules for attention\n",
    "        self.attn_matmul = DCMatMul()  # Q @ K^T\n",
    "        self.context_matmul = DCMatMul()  # attn @ V\n",
    "        \n",
    "        # Reshape/permute operations\n",
    "        self.permute_qkv = DCPermute((0, 2, 1, 3))  # (B, S, H, D) -> (B, H, S, D)\n",
    "        self.transpose_k = DCTranspose(-1, -2)  # For K^T\n",
    "        self.permute_context = DCPermute((0, 2, 1, 3))  # (B, H, S, D) -> (B, S, H, D)\n",
    "        self.contiguous = DCContiguous()\n",
    "    \n",
    "    def transpose_for_scores(self, x, batch_size):\n",
    "        \"\"\"Reshape and permute for multi-head attention.\"\"\"\n",
    "        # (B, S, H*D) -> (B, S, H, D) -> (B, H, S, D)\n",
    "        new_shape = (batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_shape)\n",
    "        return self.permute_qkv(x)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        query_layer = self.query(hidden_states)\n",
    "        key_layer = self.key(hidden_states)\n",
    "        value_layer = self.value(hidden_states)\n",
    "        \n",
    "        # 2. Reshape for multi-head attention\n",
    "        query_layer = self.transpose_for_scores(query_layer, batch_size)\n",
    "        key_layer = self.transpose_for_scores(key_layer, batch_size)\n",
    "        value_layer = self.transpose_for_scores(value_layer, batch_size)\n",
    "        \n",
    "        # 3. Attention scores: Q @ K^T\n",
    "        key_layer_t = self.transpose_k(key_layer)\n",
    "        # For DC: set the second operand\n",
    "        self.attn_matmul.set_operand(key_layer_t)\n",
    "        attention_scores = self.attn_matmul(query_layer)\n",
    "        \n",
    "        # 4. Scale by sqrt(head_dim)\n",
    "        attention_scores = self.scale_div(attention_scores)\n",
    "        \n",
    "        # 5. Softmax\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        \n",
    "        # 6. Dropout (identity in eval)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        # 7. Context: attn @ V\n",
    "        self.context_matmul.set_operand(value_layer)\n",
    "        context_layer = self.context_matmul(attention_probs)\n",
    "        \n",
    "        # 8. Reshape back\n",
    "        context_layer = self.permute_context(context_layer)\n",
    "        context_layer = self.contiguous(context_layer)\n",
    "        new_shape = (batch_size, -1, self.all_head_size)\n",
    "        context_layer = context_layer.view(*new_shape)\n",
    "        \n",
    "        return context_layer\n",
    "\n",
    "\n",
    "print(\"DCViTSelfAttention created with hookable operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test DC Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    # Create a config for testing\n",
    "    test_config = ViTConfig(\n",
    "        hidden_size=64,\n",
    "        num_attention_heads=4,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "    )\n",
    "    \n",
    "    # Create DC attention\n",
    "    dc_attn = DCViTSelfAttention(test_config)\n",
    "    dc_attn.eval()\n",
    "    \n",
    "    # Test input\n",
    "    x = torch.randn(2, 16, 64)  # (batch, seq, hidden)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = dc_attn(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # List all modules\n",
    "    print(\"\\nDC Attention Modules:\")\n",
    "    for name, module in dc_attn.named_modules():\n",
    "        if name:\n",
    "            print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full DC-Compatible ViT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCViTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Full ViT Layer with DC-decomposable operations.\n",
    "    \n",
    "    Structure:\n",
    "    - LayerNorm -> Attention -> Add (residual)\n",
    "    - LayerNorm -> MLP -> Add (residual)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention block\n",
    "        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.attention = DCViTSelfAttention(config)\n",
    "        self.attention_output = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attention_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.add_residual_1 = DCAdd()\n",
    "        \n",
    "        # MLP block\n",
    "        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_act = nn.ReLU()\n",
    "        self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.add_residual_2 = DCAdd()\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        # Attention block\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layernorm_before(hidden_states)\n",
    "        attention_output = self.attention(hidden_states)\n",
    "        attention_output = self.attention_output(attention_output)\n",
    "        attention_output = self.attention_dropout(attention_output)\n",
    "        hidden_states = self.add_residual_1(residual, attention_output)\n",
    "        \n",
    "        # MLP block\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layernorm_after(hidden_states)\n",
    "        hidden_states = self.intermediate(hidden_states)\n",
    "        hidden_states = self.intermediate_act(hidden_states)\n",
    "        hidden_states = self.output_dense(hidden_states)\n",
    "        hidden_states = self.output_dropout(hidden_states)\n",
    "        hidden_states = self.add_residual_2(residual, hidden_states)\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "print(\"DCViTLayer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    # Create and test DCViTLayer\n",
    "    test_config = ViTConfig(\n",
    "        hidden_size=64,\n",
    "        num_attention_heads=4,\n",
    "        intermediate_size=256,\n",
    "        hidden_dropout_prob=0.0,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "        layer_norm_eps=1e-6,\n",
    "    )\n",
    "    \n",
    "    dc_layer = DCViTLayer(test_config)\n",
    "    dc_layer.eval()\n",
    "    \n",
    "    x = torch.randn(2, 16, 64)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = dc_layer(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    print(\"\\nAll modules in DCViTLayer:\")\n",
    "    for name, module in dc_layer.named_modules():\n",
    "        if name and not any(c in name for c in ['attention.', 'layernorm', 'intermediate', 'output']):\n",
    "            continue\n",
    "        if name:\n",
    "            mtype = type(module).__name__\n",
    "            if not mtype.startswith('DC') and mtype not in ['Linear', 'LayerNorm', 'ReLU', 'Softmax', 'Dropout']:\n",
    "                continue\n",
    "            print(f\"  {name}: {mtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save DC Operation Modules to Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate code for dc_operations.py\n",
    "dc_operations_code = '''\"\"\"\n",
    "DC Operation Modules\n",
    "\n",
    "These modules wrap common tensor operations to make them compatible with\n",
    "hook-based DC decomposition. Each operation is implemented as a module\n",
    "that can be hooked by HookDecomposer.\n",
    "\n",
    "For most operations, pos and neg streams are processed identically since\n",
    "the operations are linear (reshape, permute, transpose, scalar mul/div).\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Optional, List, Union\n",
    "\n",
    "\n",
    "class DCReshape(nn.Module):\n",
    "    \"\"\"\n",
    "    Reshape/view module for DC decomposition.\n",
    "    \n",
    "    Reshape is a linear operation - both pos and neg streams\n",
    "    are reshaped identically.\n",
    "    \n",
    "    Args:\n",
    "        target_shape: Target shape (can include -1 for inference)\n",
    "    \"\"\"\n",
    "    _dc_is_reshape = True\n",
    "    \n",
    "    def __init__(self, *target_shape):\n",
    "        super().__init__()\n",
    "        if len(target_shape) == 1 and isinstance(target_shape[0], (list, tuple)):\n",
    "            self.target_shape = tuple(target_shape[0])\n",
    "        else:\n",
    "            self.target_shape = target_shape\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.view(*self.target_shape)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'target_shape={self.target_shape}'\n",
    "\n",
    "\n",
    "class DCDynamicReshape(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic reshape module where shape is computed at runtime.\n",
    "    \n",
    "    Use set_shape() before forward pass to set the target shape.\n",
    "    \"\"\"\n",
    "    _dc_is_reshape = True\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._target_shape: Optional[Tuple[int, ...]] = None\n",
    "    \n",
    "    def set_shape(self, *shape):\n",
    "        \"\"\"Set target shape for next forward pass.\"\"\"\n",
    "        if len(shape) == 1 and isinstance(shape[0], (list, tuple)):\n",
    "            self._target_shape = tuple(shape[0])\n",
    "        else:\n",
    "            self._target_shape = shape\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self._target_shape is None:\n",
    "            raise RuntimeError(\"Target shape not set. Call set_shape() first.\")\n",
    "        return x.view(*self._target_shape)\n",
    "\n",
    "\n",
    "class DCPermute(nn.Module):\n",
    "    \"\"\"\n",
    "    Permute dimensions module for DC decomposition.\n",
    "    \n",
    "    Permute is a linear operation - both pos and neg streams\n",
    "    are permuted identically.\n",
    "    \n",
    "    Args:\n",
    "        dims: Permutation of dimensions\n",
    "    \"\"\"\n",
    "    _dc_is_permute = True\n",
    "    \n",
    "    def __init__(self, *dims):\n",
    "        super().__init__()\n",
    "        if len(dims) == 1 and isinstance(dims[0], (list, tuple)):\n",
    "            self.dims = tuple(dims[0])\n",
    "        else:\n",
    "            self.dims = dims\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.permute(*self.dims)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dims={self.dims}'\n",
    "\n",
    "\n",
    "class DCTranspose(nn.Module):\n",
    "    \"\"\"\n",
    "    Transpose dimensions module for DC decomposition.\n",
    "    \n",
    "    Transpose is a linear operation - both pos and neg streams\n",
    "    are transposed identically.\n",
    "    \n",
    "    Args:\n",
    "        dim0: First dimension to transpose\n",
    "        dim1: Second dimension to transpose\n",
    "    \"\"\"\n",
    "    _dc_is_transpose = True\n",
    "    \n",
    "    def __init__(self, dim0: int, dim1: int):\n",
    "        super().__init__()\n",
    "        self.dim0 = dim0\n",
    "        self.dim1 = dim1\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.transpose(self.dim0, self.dim1)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim0={self.dim0}, dim1={self.dim1}'\n",
    "\n",
    "\n",
    "class DCContiguous(nn.Module):\n",
    "    \"\"\"\n",
    "    Make tensor contiguous for DC decomposition.\n",
    "    \n",
    "    This is a no-op mathematically but ensures memory layout.\n",
    "    \"\"\"\n",
    "    _dc_is_contiguous = True\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.contiguous()\n",
    "\n",
    "\n",
    "class DCScalarMul(nn.Module):\n",
    "    \"\"\"\n",
    "    Scalar multiplication for DC decomposition.\n",
    "    \n",
    "    For positive scalar s: pos_out = s * pos_in, neg_out = s * neg_in\n",
    "    For negative scalar s: pos_out = |s| * neg_in, neg_out = |s| * pos_in\n",
    "    \n",
    "    Args:\n",
    "        scalar: Scalar value to multiply by\n",
    "    \"\"\"\n",
    "    _dc_is_scalar_mul = True\n",
    "    \n",
    "    def __init__(self, scalar: float):\n",
    "        super().__init__()\n",
    "        self.register_buffer('scalar', torch.tensor(scalar))\n",
    "        self.register_buffer('is_negative', torch.tensor(scalar < 0))\n",
    "        self.register_buffer('abs_scalar', torch.tensor(abs(scalar)))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x * self.scalar\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'scalar={self.scalar.item():.6f}'\n",
    "\n",
    "\n",
    "class DCScalarDiv(nn.Module):\n",
    "    \"\"\"\n",
    "    Scalar division for DC decomposition.\n",
    "    \n",
    "    Division by scalar s is multiplication by 1/s.\n",
    "    For positive s: pos_out = pos_in / s, neg_out = neg_in / s\n",
    "    For negative s: pos_out = neg_in / |s|, neg_out = pos_in / |s|\n",
    "    \n",
    "    Args:\n",
    "        scalar: Scalar value to divide by\n",
    "    \"\"\"\n",
    "    _dc_is_scalar_div = True\n",
    "    \n",
    "    def __init__(self, scalar: float):\n",
    "        super().__init__()\n",
    "        self.register_buffer('scalar', torch.tensor(scalar))\n",
    "        self.register_buffer('is_negative', torch.tensor(scalar < 0))\n",
    "        self.register_buffer('abs_scalar', torch.tensor(abs(scalar)))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x / self.scalar\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'scalar={self.scalar.item():.6f}'\n",
    "\n",
    "\n",
    "class DCAdd(nn.Module):\n",
    "    \"\"\"\n",
    "    Element-wise addition for DC decomposition.\n",
    "    \n",
    "    (a_pos - a_neg) + (b_pos - b_neg) = (a_pos + b_pos) - (a_neg + b_neg)\n",
    "    \n",
    "    This module stores the second operand's decomposition for the hook.\n",
    "    \"\"\"\n",
    "    _dc_is_add = True\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Storage for second operand's DC decomposition\n",
    "        self._dc_operand_pos: Optional[Tensor] = None\n",
    "        self._dc_operand_neg: Optional[Tensor] = None\n",
    "    \n",
    "    def set_operand(self, b: Tensor, b_pos: Optional[Tensor] = None, b_neg: Optional[Tensor] = None):\n",
    "        \"\"\"Set the second operand for addition.\"\"\"\n",
    "        if b_pos is not None and b_neg is not None:\n",
    "            self._dc_operand_pos = b_pos\n",
    "            self._dc_operand_neg = b_neg\n",
    "        else:\n",
    "            import torch.nn.functional as F\n",
    "            self._dc_operand_pos = F.relu(b)\n",
    "            self._dc_operand_neg = F.relu(-b)\n",
    "    \n",
    "    def set_operand_decomposed(self, b_pos: Tensor, b_neg: Tensor):\n",
    "        \"\"\"Set the second operand with pre-decomposed components.\"\"\"\n",
    "        self._dc_operand_pos = b_pos\n",
    "        self._dc_operand_neg = b_neg\n",
    "    \n",
    "    def forward(self, a: Tensor, b: Tensor) -> Tensor:\n",
    "        return a + b\n",
    "\n",
    "\n",
    "class DCSplit(nn.Module):\n",
    "    \"\"\"\n",
    "    Split tensor along a dimension for DC decomposition.\n",
    "    \n",
    "    Split is a linear operation - pos and neg are split identically.\n",
    "    \n",
    "    Args:\n",
    "        split_size: Size of each split or list of sizes\n",
    "        dim: Dimension to split along\n",
    "    \"\"\"\n",
    "    _dc_is_split = True\n",
    "    \n",
    "    def __init__(self, split_size: Union[int, List[int]], dim: int = 0):\n",
    "        super().__init__()\n",
    "        self.split_size = split_size\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, ...]:\n",
    "        return torch.split(x, self.split_size, dim=self.dim)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'split_size={self.split_size}, dim={self.dim}'\n",
    "\n",
    "\n",
    "class DCChunk(nn.Module):\n",
    "    \"\"\"\n",
    "    Chunk tensor into equal parts for DC decomposition.\n",
    "    \n",
    "    Chunk is a linear operation - pos and neg are chunked identically.\n",
    "    \n",
    "    Args:\n",
    "        chunks: Number of chunks\n",
    "        dim: Dimension to chunk along\n",
    "    \"\"\"\n",
    "    _dc_is_chunk = True\n",
    "    \n",
    "    def __init__(self, chunks: int, dim: int = 0):\n",
    "        super().__init__()\n",
    "        self.chunks = chunks\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, ...]:\n",
    "        return torch.chunk(x, self.chunks, dim=self.dim)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'chunks={self.chunks}, dim={self.dim}'\n",
    "\n",
    "\n",
    "class DCCat(nn.Module):\n",
    "    \"\"\"\n",
    "    Concatenate tensors for DC decomposition.\n",
    "    \n",
    "    Concatenation is a linear operation - pos and neg are concatenated identically.\n",
    "    \n",
    "    Args:\n",
    "        dim: Dimension to concatenate along\n",
    "    \"\"\"\n",
    "    _dc_is_cat = True\n",
    "    \n",
    "    def __init__(self, dim: int = 0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, tensors: List[Tensor]) -> Tensor:\n",
    "        return torch.cat(tensors, dim=self.dim)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}'\n",
    "\n",
    "\n",
    "class DCSlice(nn.Module):\n",
    "    \"\"\"\n",
    "    Slice tensor for DC decomposition.\n",
    "    \n",
    "    Slicing is a linear operation - pos and neg are sliced identically.\n",
    "    \n",
    "    Args:\n",
    "        dim: Dimension to slice\n",
    "        start: Start index\n",
    "        end: End index (exclusive)\n",
    "    \"\"\"\n",
    "    _dc_is_slice = True\n",
    "    \n",
    "    def __init__(self, dim: int, start: Optional[int] = None, end: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        slices = [slice(None)] * x.dim()\n",
    "        slices[self.dim] = slice(self.start, self.end)\n",
    "        return x[tuple(slices)]\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, start={self.start}, end={self.end}'\n",
    "\n",
    "\n",
    "class DCDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Dropout for DC decomposition.\n",
    "    \n",
    "    In eval mode, dropout is identity.\n",
    "    In train mode, the same mask is applied to both pos and neg.\n",
    "    \n",
    "    Args:\n",
    "        p: Dropout probability\n",
    "    \"\"\"\n",
    "    _dc_is_dropout = True\n",
    "    \n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.dropout = nn.Dropout(p)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'p={self.p}'\n",
    "\n",
    "\n",
    "class DCIdentity(nn.Module):\n",
    "    \"\"\"\n",
    "    Identity module for DC decomposition.\n",
    "    \n",
    "    Useful as a placeholder or for skip connections.\n",
    "    \"\"\"\n",
    "    _dc_is_identity = True\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x\n",
    "'''\n",
    "\n",
    "print(\"DC Operations module code generated\")\n",
    "print(f\"Length: {len(dc_operations_code)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "with open('../dc_decompose/dc_operations.py', 'w') as f:\n",
    "    f.write(dc_operations_code)\n",
    "\n",
    "print(\"Saved to ../dc_decompose/dc_operations.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test All DC Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the new module\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Reload to get fresh imports\n",
    "if 'dc_decompose.dc_operations' in sys.modules:\n",
    "    del sys.modules['dc_decompose.dc_operations']\n",
    "\n",
    "from dc_decompose.dc_operations import (\n",
    "    DCReshape, DCDynamicReshape, DCPermute, DCTranspose, DCContiguous,\n",
    "    DCScalarMul, DCScalarDiv, DCAdd, DCSplit, DCChunk, DCCat, DCSlice,\n",
    "    DCDropout, DCIdentity\n",
    ")\n",
    "\n",
    "print(\"Testing DC Operations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "x = torch.randn(2, 4, 8)\n",
    "\n",
    "# Test DCReshape\n",
    "reshape = DCReshape(2, -1)\n",
    "y = reshape(x)\n",
    "print(f\"DCReshape: {x.shape} -> {y.shape}\")\n",
    "\n",
    "# Test DCPermute\n",
    "permute = DCPermute(0, 2, 1)\n",
    "y = permute(x)\n",
    "print(f\"DCPermute: {x.shape} -> {y.shape}\")\n",
    "\n",
    "# Test DCTranspose\n",
    "transpose = DCTranspose(-1, -2)\n",
    "y = transpose(x)\n",
    "print(f\"DCTranspose: {x.shape} -> {y.shape}\")\n",
    "\n",
    "# Test DCScalarMul\n",
    "scalar_mul = DCScalarMul(2.0)\n",
    "y = scalar_mul(x)\n",
    "print(f\"DCScalarMul(2.0): max_diff={torch.max(torch.abs(y - 2*x)).item():.2e}\")\n",
    "\n",
    "# Test DCScalarDiv\n",
    "scalar_div = DCScalarDiv(8.0)\n",
    "y = scalar_div(x)\n",
    "print(f\"DCScalarDiv(8.0): max_diff={torch.max(torch.abs(y - x/8)).item():.2e}\")\n",
    "\n",
    "# Test DCSplit\n",
    "split = DCSplit(4, dim=1)\n",
    "y1, = split(x)\n",
    "print(f\"DCSplit: {x.shape} -> {y1.shape}\")\n",
    "\n",
    "# Test DCChunk\n",
    "chunk = DCChunk(2, dim=-1)\n",
    "y1, y2 = chunk(x)\n",
    "print(f\"DCChunk: {x.shape} -> {y1.shape}, {y2.shape}\")\n",
    "\n",
    "# Test DCSlice\n",
    "slice_op = DCSlice(dim=1, start=1, end=3)\n",
    "y = slice_op(x)\n",
    "print(f\"DCSlice: {x.shape} -> {y.shape}\")\n",
    "\n",
    "print(\"\\nAll operations working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary: ViT Components and DC Support\n",
    "\n",
    "| Component | Module Type | DC Support | Notes |\n",
    "|-----------|-------------|------------|-------|\n",
    "| Patch Embedding | Conv2d | ✓ | Standard convolution |\n",
    "| Position Embedding | Parameter + Add | ✓ | Linear operation |\n",
    "| LayerNorm | LayerNorm | ✓ | Variance as constant |\n",
    "| Q/K/V Projection | Linear | ✓ | Weight decomposition |\n",
    "| Reshape for heads | DCReshape | ✓ | Linear operation |\n",
    "| Permute for heads | DCPermute | ✓ | Linear operation |\n",
    "| Q @ K^T | DCMatMul | ✓ | Product rule |\n",
    "| Scale by sqrt(d) | DCScalarDiv | ✓ | Linear operation |\n",
    "| Softmax | Softmax | ✓ | Jacobian backward |\n",
    "| Dropout | DCDropout | ✓ | Identity in eval |\n",
    "| attn @ V | DCMatMul | ✓ | Product rule |\n",
    "| Transpose back | DCTranspose | ✓ | Linear operation |\n",
    "| Contiguous | DCContiguous | ✓ | Memory layout |\n",
    "| Output projection | Linear | ✓ | Weight decomposition |\n",
    "| Residual add | DCAdd | ✓ | Linear operation |\n",
    "| MLP intermediate | Linear | ✓ | Weight decomposition |\n",
    "| ReLU/GELU | ReLU | ✓ | Use ReLU for DC |\n",
    "| MLP output | Linear | ✓ | Weight decomposition |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary complete!\")\n",
    "print(\"\\nAll ViT components can be decomposed with the implemented DC modules.\")\n",
    "print(\"\\nKey files created/updated:\")\n",
    "print(\"  - dc_decompose/dc_operations.py: Linear operations (reshape, permute, etc.)\")\n",
    "print(\"  - dc_decompose/dc_matmul.py: Matrix multiplication\")\n",
    "print(\"  - dc_decompose/hook_decomposer.py: LayerNorm, Softmax, etc.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
